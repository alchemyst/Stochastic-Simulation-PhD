\chapter{Optimisation}

\section{Background}
Introduce optimisation quickly to show I know what I'm talking about, then move swiftly to 

\section{Important definitions}

\subsection{Optima}
\subsection{Global optima}
\subsection{Pareto efficiency}

From Wikipedia
\begin{quote}
Consider a design space with $n$ real parameters, and for each design-space point there are $m$ different criteria by which to judge that point. Let $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$ be the function which assigns, to each design-space point $x$, a criteria-space point $f$($x$). This represents the way of valuing the designs. Now, it may be that some designs are infeasible; so let $X$ be a set of feasible designs in ${\mathbb{R}}^n$, which must be a [[compact space|compact set]]. Then the set which represents the feasible criterion points is $f$($X$), the image of the set $X$ under the action of $f$. Call this image $Y$.

Now construct the Pareto frontier as a subset of $Y$, the feasible criterion points. It can be assumed that the preferable values of each criterion parameter are the lesser ones, thus minimizing each dimension of the criterion vector. Then compare criterion vectors as follows: One criterion vector $y$ $strictly dominates$ (or "is preferred to") a vector $y*$ if each parameter of $y$ is no greater than the corresponding parameter of $y*$ and at least one parameter is strictly less: that is, $\mathbf{y}_i \le \mathbf{y*}_i$ for each $i$ and $\mathbf{y}_i < \mathbf{y*}_i$ for some $i$. This is  written as $\mathbf{y} \succ \mathbf{y*}$ to mean that $y$ strictly dominates $y*$. Then the Pareto frontier is the set of points from $Y$ that are not strictly dominated by another point in $Y$.
\end{quote}

\subsection{Curve fitting and approximation}
Here we introduce the general curve fitting problems.

\section{Population based methods}
Cut stuff about PSO here.

\section{Multiple objectives}
MOPSO.

\subsection{Multi-objective optimisation}
The trade-off between accuracy and generality of a fit would
traditionally be decided by the designer of an algorithm.  Perhaps
some noise reduction would be done before identifying events, or
constraints on the fitting functions would be enforced to avoid over
fitting~\cite{Arora2003Fitting,Punskaya2002Bayesian}.  

Multi-objective optimisation provides a different approach.  All the
objective function values are evaluated and a solution is retained if
it is better in any way than all of the solutions already encountered.
Such solutions are called Pareto optimal or nondominated
solutions \cite{Steuer1986Multiple}. The result of such an optimisation
algorithm is a \emph{list} of Pareto optimal solutions, or more
properly an approximation of the Pareto front.  This list is most
commonly called the archive.

\subsection{Evolutionary algorithms}
Evolutionary algorithms are a natural fit for multi-objective
optimisation, as they are already population based.  Genetic
algorithms in particular have enjoyed
popularity~\cite{Deb2001MultiObjective}.  Recent work in Particle
Swarm Optimisation has rekindled interest in using it for
multi-objective optimisation.

The algorithm used in this work is the MOPSO-CD (Multi-Objective
Particle Swarm Optimisation with Crowding Distance) algorithm proposed
by \cite{Raquel2005Effective}.  It is a modification of Particle Swarm
Optimisation that adds an archive of nondominated solutions and uses a
crowding distance measure to prevent many similar Pareto optimal
solutions from being retained in the archive.


% Local Variables:
% TeX-master: "thesis"
% End:

