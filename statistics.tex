\chapter{Stationary and signal statistics}\label{chap:stats}
\begin{overview} 
  Deterministic models of chemical engineering systems assume that all the variables are known exactly.
  However, uncertainty typically exists in the model paramters, the inputs to the model or the measurements from the model.
  This chapter summarises the nomenclature used in the rest of the work in addition to covering the theory used to incorporate uncertainty into models.
\end{overview}

\section{Random Phenomena}
\label{sec:statistisc:probability}
Simple random phenomena are commonly described using a model consisting of the following elements:\citep[1]{kulkarni1999modeling}
\begin{description}
\item[Sample space] Denoted $\Omega$, giving the set of all possible outcomes of the random phenomenon.  
  Particular outcomes are denoted $\omega$.
\item[Events] An event (typically denoted $E_i$) is a subset of the sample space.
\item[Event probabilities] Are written as $\prob{E}$ and are numbers between $0$ and $1$ representing the likelihood of occurrence of the event $E$.
\end{description}

Additionally, $\prob{E_2|E_1}$  represents the \emph{conditional probability} of event $E_2$ given that $E_1$ has indeed occurred.

It is clear that $\prob{\Omega}=1$, that is there is a 100\% probability of something in the set of all possible outcomes occurs.
Also, $0 \leq \prob{E_i} \leq 1$ for any $E_i$ that is a subset of $\Omega$.

\section{Univariate random variables}
\label{sec:univ-rand-vari}
Random variables are typically defined as the functions that map the sample space of a random phenomenon to a real number.
Examples are the value of a particular temperature or pressure or the total number of heads shown after flipping a certain number of coins.

\subsection{Cumulative distribution function}
It is often useful to refer to the cumulative distribution function of a random variable, defined as 
\begin{equation}
  \label{eq:cdf}
  F(x) = \prob{X \leq x}, x \in (-\infty,\infty)
\end{equation}

Due to the fact that a random variable must take on a particular value in $(-\infty,\infty)$, $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1$.
$F$ must also be nondecreasing ($x \leq y \implies F(x) \leq F(y)$) and right continuous ($\lim{\epsilon \to 0+} F(x+\epsilon) = F(x)$).

Random variables are called discrete if they have CDFs that contain discrete steps.  
Such discrete random variables can be characterise by their state space $S = {x_0, x_1, x_2, \dots}$.

\subsection{Probability functions}
For discrete random variables, the function $p_k = \prob{X=x_k}$ for $k \geq 0$, with $x_k \in S$ is called the probability mass function (PMF).

Random variables with for which a function $f$ exist such that 
\begin{equation}
  \label{eq:cdffrompdf}
  F(x) = \int_{-\infty}^xf(u)\dd u \quad \forall x \in \mathbb{R}
\end{equation}
are called continuous random variables.
If $F$ is continuous and piecewise differentiable, then $f$ completely determines $F$ and is called the probability distribution function (PDF) of the variable $X$.
Important properties of PDFs include $f(x) \geq 0 \quad \forall x \in \mathbb{R}$ (as no negative probability integrals are allowed) and $\int_{-\infty}^{\infty} f(u) \dd u = 1$ (due to the limit on $F$).  
It is however important to note there is no upper bound on $f$.

\subsection{Expectation}
The expected value of a random variable $X$, denoted $\expect{X}$ is calculated similarly for discrete and continuous variables by summing over probability functions.  
For discrete variables, a sum is taken over the probability mass function as follows:
\begin{equation}
  \label{eq:discreteexpectation}
  \expect{X} = \sum_{k=0}^{\infty} x_k\prob{X = x_k} = \sum_{k=0}^{\infty} x_kp_k
\end{equation}
while for continuous variables,
\begin{equation}
  \label{eq:continuousexpectation}
  \expect{X} = \int_{-\infty}^{\infty}xf(x) \dd x
\end{equation}

By analogy to weighted means, the expected value of a random variable is often also called the mean of its PDF, denoted $\mu$.

\subsubsection{Median and mode}
The expectation or mean is one characterisation of a measure of central tendency of the PDF.
% http://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php
Other measures are the median and the mode.
The median can be defined as the value $m$ such that $\prob{x<m}=\frac{1}{2}$ and interpreted as the value that splits the PDF into two equal area parts.
The mode is a value that maximises $f(x)$, loosely the peak of $f(x)$ or the most likely value.

There is no restriction on these values being the same, but many commonly used distributions are symmetrical about the median and feature modes equal to their means.

%TODO: Picture showing a PDF with mean, median, mode.

\subsubsection{Moments}
Equation~\ref{eq:expectation} can be seen as a specific case of a more general concept called the moment.
\begin{quote} % wikipedia
The $n$\textsuperscript{th} moment of a real-valued continuous function $f(x)$ of a real variable $x$ about a value $c$ is
\begin{equation}
\label{eq:moment}
\mu'_n = \expect{X^n} = \int_{-\infty}^\infty (x - c)^n f(x) \dd x
\end{equation}
\end{quote}

Usually when no additional information is given, ``the moment of a function'' means the moment about $c=0$.
Thus, the first moment ($n=1$) of a PDF gives the expected value of a random variable with that PDF.

Moments that do not converge to a real value are said not to exist.
If the $n$\textsuperscript{th} moment exists, the lower-order moments are guaranteed to exist.\citehere

Moments about the mean of a PDF are called central moments and have special interpretations.  
The zeroth central moment must be 1 due to the requirements of the CDF, while the first central moment must be zero per definition.

The second central moment is the also called the variance, which is a measure of how wide the PDF is.
The positive square root of the variance is the standard deviation, $\sigma$.
The standardised or normalised moment is the central moment divided by $\sigma^n$.

The third and fourth central moments are usually reported in standardised form and given the names skewness ($\gamma$) and kurtis ($\kappa$).
\nomenclature[ga]{$\gamma$}{Skewness}
\nomenclature[ga]{$\kappa$}{Kurtosis}

The skewness is a measure of how far the median is away from the mean.
Kurtosis is a measure of how closely the distribution is spread around the mean value.

\section{Multivariate random variables}
All the properties discussed in section~\ref{sec:univ-rand-vari} can be extended to handle more than one value \cite[65]{kulkarni1999modeling}.
A vector can be formed from several random variables $X_i$ to form a multivariate random variable $\vect{X}$.
Variables that are related in this way are called jointly distributed random variables.

\subsection{Distribution functions}
The cumulative distribution function and probability distribution functions are defined in similar ways, as 
\begin{equation}
  \label{eq:mvcdf}
  F(\vect{x}) = \prob{X_1=x_1, X_2=x_2,\dots, X_n = x_n}
\end{equation}
and
\begin{align}
  \label{eq:mvpdf}
  F(\vect{x}) &= \int_{-\infty}^{x_n}\cdots\int_{-\infty}^{x_2}\int_{-\infty}^{x_1} f(x \dd x_1 \dd x_2 \dots \dd x_n \\
              &= \int_V f(\vect{x}) \dd \vect{x}
\end{align}
where $V$ represents the volume in the state space where the first integral is evaluated.
%FIXME: Discrete distributions aren't discussed.

It is useful to define a marginal distribution as the distribution of a particular random variable within a multivariate random variable.  
The marginal CDF of a variable $X_i$ is written $F_{X_i}(x_i) = \prob{X_i \leq x_i}$ and can be derived from the joint CDF by evaluating $F_{X_i}(x_i) = F(\infty,\infty,\dots,x_i,\dots,\infty,\infty)$

\subsection{Independance}
The jointly distributed random variables $\vect{X}$ are said to be independant if 
\begin{equation}
  \label{eq:independance}
  F(\vect{x}) = \prod_i^n F_{X_i}(x_i)
\end{equation}

If in addition to being independant, the random variables share the same marginal distributions, they are said to be independantly identically distributed (IID) variables.

\subsection{Mean}
The mean or expectation of a multivariate random variable is simply a vector, $\vect{mu}$, containing the means of the marginal distributions.

\subsection{Variance and covariance}
Generalising the scalar definition of the variance is slightly more complicated.
The covariance matrix can be defined as 
\begin{equation}
  \label{eq:covariance}
  \Sigma_{ij} = \cov(X_i, X_j) = \expect{(X_i-\mu_i)(X_j-\mu_j)}
\end{equation}
or equivalently in matrix form:
\begin{equation}
  \label{eq:matrixcovariance}
  \Sigma = \expect{(\vect{X} - \vect{\mu})(\vect{X}-\vect{\mu})\transpose}
\end{equation}

%TODO: find some references
There is some difference in notation, as $\Sigma(\vect{X})$ is called either the variance or the covariance matrix.
It should be clear that the diagonal elements of $\Sigma$ correspond to $\sigma_i^2$, the variance of the individual 

\section{Stochastic processes}
\label{sec:stochastic-processes}
Stochastic processes can be defined as the sequence of random variables ${X_0, X_1,\dots}$ that are measured as a process evolves over time \citep[107]{kulkarni1999modeling}.
For discrete time, the sequence of variables is also called a time series.
The state space of the process, $S$, is the set of values that $X_n$ can take for any $n$.

\subsection{Discrete-time Markov Processes}
A stochastic process with state space $S$ has the Markov property if the current state completely determines the probability of the following state.
A sequence $X_1,X_2, \dots ,X_t$ having this property is known as a Markov chain.

Stated mathematically, a Markov chain obeys the property
\begin{equation}
  \label{eq:markovproperty}
  \prob(X_{t+1} = j|X_{t}=i) = \prob(X_{m+1}=j|X_{m}=i)=p_{ij}
\end{equation}
in words, the probability that the next state will be equal to $j$ given that the current state is $i$ is only dependant on the current state.

When $S$ is a countable set, the state transition probabilities can be written  as a state transition matrix $P$ as shown for a 3 state process in equation~\ref{eq:markovmatrix}
\begin{equation}
\label{eq:markovmatrix}
P = \left[ 
  \begin{array}{cccc}
    p_{11} & p_{12} & p_{13}\\
    p_{21} & p_{22} & p_{23}\\
    p_{31} & p_{32} & p_{33}\\
  \end{array} \right ]
\end{equation}

The probability of remaining within the state space must be unity, hence we may write 
\begin{equation}
  \label{eq:rowsumone}
  \sum_{j\in S} p_{ij}=1~\forall~i \in S.
\end{equation}

Matrices with this property as well as the common-sense property that $0 \leq p_{ij} \leq 1$ (as they are probabilities) are called stochastic matrices.

The orientation of $P$ is not unique. 
The arrangement with the current state in the rows and next state in the columns is known as a right transition matrix. 
The transpose arrangement has also been used (see for instance \citet{bhar.hamori2004hidden}) and is then described as a left transition matrix. 
Modern engineering usage leans toward the description used in this work.

A common way of visualising a Markov process with countable state space is by showing a directed graph with the states in the nodes and the transition probabilities on the edges as shown in Figure~\ref{fig:markovgraph}.
In these representations, it is customary to neglect edges with zero probabilities.

\begin{figure}[htbp]
  \centering
  \begin{minipage}{0.4\textwidth}
    \includegraphics[scale=0.5]{smallmarkov}
  \end{minipage}
  \begin{minipage}{0.4\textwidth}
   $\displaystyle P = \left [ 
      \begin{array}{ccc} 
        0.1 & 0.9 & 0 \\ 
        0 & 0.1 & 0.9 \\ 
        0.2 & 0.8 & 0 
      \end{array} \right ]$
  \end{minipage}
  \caption{Markov process represented by a transition matrix and a graph}
  \label{fig:markovgraph}
\end{figure}

Figure~\ref{fig:markovtimeseries} shows the result of simulation of the Markov process shown in figure~\ref{fig:markovgraph} over 100 iterations.  
It is clear that the second and third states are more likely than the first.
One might reasonably enquire what the overall likelihood of a particular state would be. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{simplemarkov_100_states}
  %TODO: Convert to PGF
  \caption{Time series generated by the simulation of a discrete-time Markov Model}
  \label{fig:markovtimeseries}
\end{figure}

The state transition probabilities sufficiently describe the time dependence of the process, but the initial state can not be determined using the transition probabilities alone.
The probability of the process starting out in a given state $i$ is denoted $\pi_i, i \in S$, and the vector of initial state probabilities is called
$\vect{\pi}$.
It can be seen that a discrete time Markov process is completely described by its state space $S$, its state transition matrix $P$ and its initial state probability vector $\vect{\pi}$.  
If $S$ is countable and has $N$ elements, $N^2 + N$ probabilities have to be known to fully characterise the process.
For convenience, the model is written $\lambda = (P, \vect{\pi})$.

\subsection{Transient and limiting behaviour}


\subsection{Hidden Markov Models} 
It is not always possible to observe the state (in the state space $S$) of a Markov process directly. 
It may, however, be possible to make observations from an observation space $O$ related to the state of the process.
If the probability of making a particular observation is only related to the current state of the process, the process may be described by a hidden Markov model (HMM).
What is ``hidden'' in this case are the true values of the Markov process states.

Figure~\ref{fig:hiddenmarkov} shows the situation graphically.
If the Markov process is in state 1, there are even odds that observation 2 or 3 will be made.
In state 2, only observation 2 is made and state 3 is associated with observation 1 80\% of the time and observation 2 20\% of the time.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5,clip,trim=0 4cm 0 0]{smallhiddenmarkov}
  \caption{Graphical representation of a finite hidden Markov model}
  \label{fig:hiddenmarkov}
\end{figure}

The probabilities associated with a observation $k$ being made when the process is in state $i$ may be written $b_{ik}$ and can be arranged into an observation probability matrix $B$ in a similar fashion as was previously done for $P$. 
One difference is that, while $P$ was an $N \times N$ square matrix, $B$ will have $N$ rows associated with the $N$ states of the Markov process and $M$ columns associated with the observation space. 
An HMM as described here can therefore be characterised by the same $N^2+N$ probabilities describing the Markov process in addition to $MN$ observation probabilities.  
The model description can be abbreviated to $\lambda = (P, B, \vect{\pi})$.

There are three main problems associated with HMMs \citep{gamerman.lopes2006markov}:
\begin{itemize}
\item What is the probability of generating a specific output sequence from a particular model?
\item What is the most likely sequence of states that would lead to a particular output sequence? 
\item How do we identify the model that corresponds to a given output sequence?
\end{itemize}

\subsection{Continuous-time Markov Processes}
The description of discrete-time Markov processes assumed that the transition time was known or unimportant and one could imagine simulating the process by picking a state $i$ and moving to the next state with probability $p_{ij}$.  
One shortcoming of such a description is that there is no information about the amount of time a process remains in a particular state before moving to the next (or possibly same) state.  

% TODO: Flesh out this description
% look at perhaps http://www.stat.sfu.ca/~lockhart/richard/380/00_3/lectures/19/web.html
Continuous-time Markov processes encode the transition probabilities as transition rates $q_{ij}$ (forming a $Q$ matrix as $p_{ij}$ formed a $P$ matrix), such that
\begin{equation}
  \label{eq:contmarkov}
  \prob(X(t+\Delta t) = j | X(t) = i) = 
  \begin{cases}
    1 - q_{ii}\Delta t + o(\Delta t) & \text{for } i = j \\
    q_{ij}\Delta t + o(\Delta t) & \text{otherwise}
  \end{cases}
\end{equation}

The idea is that, having changed to a particular state the probability of moving to the next one approaches one over time, but at different rates.

\section{Sample Statistics}
All the properties discussed up to now have been assumed to be known values or directly calculable from known values.
In practice, however, it is common to come across data thought to be the particular values that random variables have taken on or the evolution of a stochastic process.
In such cases, it is desirable to estimate the properties discussed based on a (hopefully representative) sample.
The sampled values available will be written $\theta_i$.

The problems that present themselves are therefore
\begin{itemize}
\item Estimation of each of the stationary statistics discussed in sections~\ref{sec:univ-rand-vari} and \ref{sec:stochastic-processes}.
\item Determination of the validity of these estimates
\end{itemize}

\subsection{Estimation}
\subsubsection{Bias}
% http://en.wikipedia.org/wiki/Bias_of_an_estimator
When a small sample is taken from a distribution, the properties of this sample are likely to deviate from the from the properties of the distribution.
Different methods of calculation can attempt to make the estimates better with regards to some criterion.
Systematic deviations from the true values are described as bias and estimates that have been developed to counter a particular deviation are called unbiased estimators.

\subsubsection{Mean}
The mean $\bar{\theta}$ of a distribution may be estimated from samples $\theta_i$ by computing the arithmetic mean
\begin{equation}
  \label{eq:mean}
  \bar{\theta} = \frac{\displaystyle \sum_{i=0}^N \theta_i}{N}
\end{equation}
This is an unbiased estimator of the mean.

\subsubsection{Variance or standard deviation}
The sample standard deviation (written $s_N$ for a sample size of $N$) can be naively calculated by analogy to the defintion of the variance as
\begin{equation}
  \label{eq:samplestandardeviation}
  s_N = \sqrt{}
\end{equation}

\subsubsection{Skewness}
The skewness estimator is given by \citet{mooney1997monte} as:
\begin{equation} 
  \sqrt{\beta_1} =
  \frac{\displaystyle\sum_{i=1}^t \left ( \theta_i - \bar{\theta} \right
    )^3/t} { \left [ \displaystyle\sum_{i=1}^t \left (
        \theta_i-\bar{\theta} \right )^2/t \right ]^\frac{3}{2}} 
\end{equation}
\nomenclature[ga]{$\beta_1$}{Skew estimator} 
\nomenclature[ga]{$\theta$}{Vector of samples for statistical tests}

\subsubsection{Kurtosis}
The skewness estimator is usually combined with a kurtosis estimator 
\begin{equation} 
  \beta_2 =
  \frac{\displaystyle\sum_{i=1}^t \left ( \theta_i - \bar{\theta} \right
    )^4/t} { \left [ \displaystyle\sum_{i=1}^t \left(
        \theta_i-\bar{\theta} \right)^3/t \right]^2} 
\end{equation}
\nomenclature[ga]{$\beta_2$}{Kurtosis estimator}
%
In both these equations, $\bar{\theta}$ is the arithmetic mean of the
samples (ie, the sum of the elements divided by $t$).  Values of 0 and
3 for the skewness and kurtosis respectively are expected for a normal
distribution~\citep{kleijnen1975statistical}.

\section{Estimating state transition probabilities}
The most direct method of estimating the state transition probabilities of a Markov process is to count the number of transitions in an input signal.
This strategy has some problems:
\begin{enumerate}
\item Certain transitions may not occur in the input signal, so that these transitions will never be simulated by the identified model
\item Segmentation of the input signal may bias the event types or transitions -- if a certain event is more often fit by the segmentation algorithm, that event will be over-represented in the transition matrix.
\end{enumerate}

If transitions between some events are very rare, it may be advisable to introduce a small artificial probability into the matrix to ensure that the event has a chance of  getting generated during the simulation.
This is especially true if the repercussions of a certain event combination are significant.  

Segmentation bias can be combated by generating a large unbiased test set and testing the segmentation algorithm on it.
If a segmentation bias is detected, the transition probabilities can be modified to take these into account.


\subsection{Verification}
It is often desired to test whether a particular sample has properties that one would expect of one were to sample a random variable with a particular CDF or PDF.
The most common tests are for normality, in other words whether the sample seems to have been drawn from a normal distribution.

\subsubsection{Shapiro-Wilk test for normality}\label{sec:shapiro-wilk-test}
The Shapiro-Wilk test~\citep{shapirowilk}, calculates a W statistic
that tests whether a random sample, $\theta_1, \theta_2, \dots,
\theta_n$ comes from (specifically) a normal distribution.  Small
values of W are evidence of departure from normality.  This test has
done very well in comparison studies with other goodness of fit tests.

The W statistic is calculated as follows: 
\begin{equation} 
  \label{eq:shipirowilk} 
  W = \frac{
    \left( \displaystyle \sum_{i=1}^n {a_i \times \theta_i} \right)^2}
  {\displaystyle \sum_{i=1}^n 
    \left ( \theta_i - \bar{\theta} \right )^2}
\end{equation}
where the $\theta_i$ are the ordered sample values ($\theta_1$ is the
smallest) and the $a_i$ are constants generated from the means,
variances and covariances of the order statistics of a sample of size
$n$ from a normal distribution.

\subsubsection{Kolmogorov-Smirnov test}\label{sec:kolm-smirn-test}
Further checking of normality can be done by using the
Kolmogorov-Smirnov test~\citep[392--394]{chakravartistat} to compare
the distribution to a normal distribution with the same mean and
standard deviation.  This is not a specific test of normality, but
rather a general goodness-of-fit test for any probability
distribution.

The cumulative probability distribution curves for both distributions
are plotted, and the maximum distance between them determined.  This
distance (the Kolmogorov-Smirnov distance or the KS statistic) is then
compared to tables for the number of samples in the test distribution
to determine the goodness of fit.  The critical distance is also
affected by the level of confidence, $\alpha$.  It is customary to set
$\alpha=\num{0.05}$, corresponding to a 5\% chance of mistakenly
discarding the assumption that the test distribution is indeed
normally distributed.
\nomenclature[ga]{$\alpha$}{Level of confidence in normality tests}

For samples larger than 20, the critical distance is found by
calculating an asymptotic solution to an $n$\textsuperscript{th} order
polynomial.  This task is usually handled by computer software.

The Kolmogorov-Smirnov test was favoured for confirming normality of
the test results as it lends itself well to graphical interpretation,
enabling the tester to interpret the normality results more
meaningfully than a simple number.

\section{Signal statistics}


\subsection{IEC 61131-3}
IEC 61131-3 is a standard that defines

\url{http://en.wikipedia.org/wiki/IEC_61131-3}

\subsection{Input identification}
Stochastic simulation of systems, especially those using Monte Carlo methods, require good input scenarios to generate good output data.
It is common to make use of Markov processes to generate realistic inputs based on historic data.
However, identification of ``events'' within historic data can be troublesome.  
Much work has been done on identification of events or trends in data (\citet{maurya.rengaswamy.ea2007fault} give a good overview of trend analysis techniques).  
Reducing process signals to symbols representing qualitative event types rather than quantitative data allows patterns to be found in events, or what \citet{cheung.stephanopoulos1990representation} refer to as episodes.  
In this seminal work, the authors define a formal language in terms of the 7 primitives shown in figure~\ref{fig:stephanopoulosprimitives}.

\begin{figure}[htbp]
  \centering
  \setlength{\unitlength}{0.7em}
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \put(7,1){D}
    \put(4,4){$(+,-)$}
    \thicklines
    \qbezier(1,1)(2,7)(7,7)
  \end{picture}
  }
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \put(7,1){B}
    \put(1,4){$(+,+)$}
    \thicklines
    \qbezier(1,1)(7,2)(7,7)
  \end{picture}
  }
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \thicklines
    \qbezier(1,1)(2,2)(7,7)
    \put(7,1){C}
    \put(1,5){$(+,0)$}
  \end{picture}
  }
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \thicklines
    \qbezier(1,7)(6,7)(6.7,1)
    \put(7,1){G}
    \put(1,4){$(-,-)$}
  \end{picture}
  }
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \thicklines
    \qbezier(1,7)(2,2)(6.7,1)
    \put(7,1){E}
    \put(4,4){$(-,+)$}
  \end{picture}
  }
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \thicklines
    \qbezier(1,7)(4,4)(7,1)
    \put(7,1){F}
    \put(5,5){$(+,-)$}
  \end{picture}
  }
  {\small
  \begin{picture}(8,8)
    \put(0,0){\line(1,0){8}}
    \put(0,0){\line(0,1){8}}
    \thicklines
    \qbezier(1,4)(3,4)(7,4)
    \put(7,1){A}
    \put(4,6){$(0,0)$}
  \end{picture}
  }
  \caption[Episodic analysis primitives]{Episodic analysis primitives according to \citet{cheung.stephanopoulos1990representation}.}
  \label{fig:stephanopoulosprimitives}
\end{figure}

One problem with event-based approaches is that, to estimate the likelihood of a state transition, at least one such a transition has to be identified in the training data.  
This means that such processes are usually very data-intensive.  
The same holds for episodic analysis -- some patterns may go unnoticed because of misfitting. 
Furthermore, it is difficult to determine an objective function for fitting, as attempts to fit the data too accurately usually lead to a loss of generality (the over-fitting problem, described by \citet{arora.khot2003fitting} and others).

\section{Uncertainty}
The reason for stochastic simulation is the existence of uncertainty in one or more aspects of the model.  
If the model was perfect, a single deterministic simulation would be a complete exploration of the model.  
In the steady state case, this means that solving the steady state equations yield a single, reliable result.  

Uncertainty can be classified according its location as follows.

\subsection{Input uncertainty}
Input uncertainty refers to uncertainty about the values of the inputs into the model.  
In control problems and when processing natural products, the properties of input streams may not be known in more detail than expected ranges.

Input uncertainties are further subdivided into quantities that have known distributions rather than fixed values and quantities that vary over time, exhibiting known events.

\subsubsection{Input types}

\subsection{Parametric}
Parametric uncertainty is uncertainty in parameters of the model.  
It is usually assumed that the model shape is correct, but that differences between simulated values and experimental data can be explained by inaccurate parameter values.  
Parametric uncertainty can be due to incorrect models, which do not model variations accurately, or to inaccurate measurements.  
If model parameters change over time in predictable ways, it is more proper to model these changes by introducing more model equations than to mask them by assuming parametric uncertainty.  
The specific combination of parameters for a simulation must therefore remain constant for that simulation.

A process for which the parameters remain constant in this way is called an ergodic process
% http://en.wikipedia.org/wiki/Ergodic_process

It is possible to estimate the mean of an ergodic process by 
\begin{equation}
  \hat{\mu_T} = \frac{1}{2T} \int_{-T}^{T} x(t) \dd t
\end{equation}


\subsection{Model uncertainty}
Model uncertainty is the hardest to handle during simulations, as this refers to uncertainty in the form of the model equations.  
Model uncertainty is different from parametric uncertainty as it implies that no combination of parameters in the model can accurately capture the behaviour of the system.  
The boundary between model and parametric uncertainty is often blurred by the fact that models are often developed with additional parameters designed to make the model more flexible.  
This work does not address model uncertainty.

% Local Variables:
% TeX-master: "thesis"
% End:

